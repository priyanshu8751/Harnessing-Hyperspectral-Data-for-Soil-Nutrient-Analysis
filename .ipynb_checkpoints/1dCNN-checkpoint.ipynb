{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24fd41f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERAL UTILITIES\n",
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from  tqdm.notebook import tqdm\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# MODEL DEVELOPMENT DEPENDENCIES\n",
    "import numpy as np\n",
    "import pywt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a0bfc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gt(file_path: str):\n",
    "    \"\"\"Load labels for train set from the ground truth file.\n",
    "    Args:\n",
    "        file_path (str): Path to the ground truth .csv file.\n",
    "    Returns:\n",
    "        [type]: 2D numpy array with soil properties levels\n",
    "    \"\"\"\n",
    "    gt_file = pd.read_csv(file_path)\n",
    "    labels = gt_file[[\"P\", \"K\", \"Mg\", \"pH\"]].values\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f920ab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_1d(directory, gt_file_path):\n",
    "  x_train = []\n",
    "  y_train = []\n",
    "  x_test = []\n",
    "  y_test = []\n",
    "\n",
    "  labels = load_gt(gt_file_path)\n",
    "\n",
    "  all_files = np.array(\n",
    "      sorted(\n",
    "          glob(os.path.join(directory, \"*.npz\")),\n",
    "          key=lambda x: int(os.path.basename(x).replace(\".npz\", \"\")),\n",
    "      )\n",
    "  )\n",
    "\n",
    "  train_size = 0.8\n",
    "  val_size = 0.9\n",
    "\n",
    "  for idx, file_name in tqdm(enumerate(all_files),total=len(all_files), desc=\"Loading {} data ..\"\n",
    "                              .format(\"training\")):\n",
    "      # We load the data into memory as provided in the example notebook of the challenge\n",
    "      with np.load(file_name) as npz:\n",
    "          mask = npz[\"mask\"]\n",
    "          data = npz[\"data\"]\n",
    "          sh = data.shape[1:]\n",
    "            \n",
    "          augmented_data = []\n",
    "          for x in range(0, sh[0]):\n",
    "            for y in range(0, sh[1]):\n",
    "              if mask[0][x][y] == False:\n",
    "                augmented_data.append(data[:, x, y])\n",
    "          \n",
    "          for i in range(len(augmented_data)):\n",
    "            if (i / len(augmented_data) < train_size):\n",
    "              x_train.append(augmented_data[i])\n",
    "              y_train.append(labels[idx])\n",
    "            else:\n",
    "              x_test.append(augmented_data[i])\n",
    "              y_test.append(labels[idx])\n",
    "\n",
    "  return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15009e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_1d_kf(directory, gt_file_path):\n",
    "  x_train = []\n",
    "  y_train = []\n",
    "  x_val = []\n",
    "  y_val = []\n",
    "  x_test = []\n",
    "  y_test = []\n",
    "\n",
    "  labels = load_gt(gt_file_path)\n",
    "\n",
    "  all_files = np.array(\n",
    "      sorted(\n",
    "          glob(os.path.join(directory, \"*.npz\")),\n",
    "          key=lambda x: int(os.path.basename(x).replace(\".npz\", \"\")),\n",
    "      )\n",
    "  )\n",
    "\n",
    "  train_size = 0.8\n",
    "  val_size = 0.9\n",
    "\n",
    "  for idx, file_name in tqdm(enumerate(all_files),total=len(all_files), desc=\"Loading {} data ..\"\n",
    "                              .format(\"training\")):\n",
    "      # We load the data into memory as provided in the example notebook of the challenge\n",
    "      with np.load(file_name) as npz:\n",
    "          mask = npz[\"mask\"]\n",
    "          data = npz[\"data\"]\n",
    "          sh = data.shape[1:]\n",
    "            \n",
    "          augmented_data = []\n",
    "          for x in range(0, sh[0]):\n",
    "            for y in range(0, sh[1]):\n",
    "              if mask[0][x][y] == False:\n",
    "#                 first_diff = np.diff(data[:, x, y])\n",
    "#                 second_diff = np.diff(data[:, x, y], n=2)\n",
    "\n",
    "#                 # Pad the arrays with zeros to make their lengths compatible with the original array\n",
    "#                 first_diff_padded = np.pad(first_diff, (1, 0), mode='constant')\n",
    "#                 second_diff_padded = np.pad(second_diff, (2, 0), mode='constant')\n",
    "\n",
    "#                 # Concatenate the original array with the differentiated arrays\n",
    "#                 new_array = np.concatenate((data[:, x, y], data[:, x, y] + first_diff_padded, data[:, x, y] + second_diff_padded))\n",
    "                augmented_data.append(data[:, x, y])\n",
    "          \n",
    "          for i in range(len(augmented_data)):\n",
    "              x_train.append(augmented_data[i])\n",
    "              y_train.append(labels[idx])\n",
    "                \n",
    "#           print(augmented_data[0][149])\n",
    "\n",
    "  return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02a707cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_testing(directory):\n",
    "  x_test = []\n",
    "\n",
    "  all_files = np.array(\n",
    "      sorted(\n",
    "          glob(os.path.join(directory, \"*.npz\")),\n",
    "          key=lambda x: int(os.path.basename(x).replace(\".npz\", \"\")),\n",
    "      )\n",
    "  )\n",
    "\n",
    "  train_size = 0.8\n",
    "  val_size = 0.9\n",
    "\n",
    "  for idx, file_name in tqdm(enumerate(all_files),total=len(all_files), desc=\"Loading {} data ..\"\n",
    "                              .format(\"training\")):\n",
    "      # We load the data into memory as provided in the example notebook of the challenge\n",
    "      with np.load(file_name) as npz:\n",
    "          mask = npz[\"mask\"]\n",
    "          data = npz[\"data\"]\n",
    "          sh = data.shape[1:]\n",
    "            \n",
    "          augmented_data = []\n",
    "          for x in range(0, sh[0]):\n",
    "            for y in range(0, sh[1]):\n",
    "              if mask[0][x][y] == False:\n",
    "                augmented_data.append(data[:, x, y])\n",
    "                \n",
    "          augmented_data = np.array(augmented_data)\n",
    "          augmented_data = scaler.transform(augmented_data)\n",
    "          pred = model.predict(augmented_data)\n",
    "          x_test.append(np.mean(pred, axis=0))\n",
    "    \n",
    "  x_test = np.array(x_test)\n",
    "  return x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74301aed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b459fdd11e46dab883cf9ec95f0af8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading training data ..:   0%|          | 0/1732 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Please be sure that the directory and file locations are given correctly in your own system\n",
    "train_data_dir = \"train_data/train_data/train_data\"\n",
    "test_data_dir = \"test_data\"\n",
    "gt_data_path = \"train_data/train_data/train_gt.csv\"\n",
    "\n",
    "# Loading training raw data\n",
    "# X_train = load_data(train_data_dir, gt_data_path)\n",
    "# X_train, Y_train, X_val, Y_val, X_test, Y_test = load_data_1d(train_data_dir, gt_data_path)\n",
    "# X_train, Y_train, X_test, Y_test = load_data_1d(train_data_dir, gt_data_path)\n",
    "X_train, Y_train = load_data_1d_kf(train_data_dir, gt_data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51795a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "# X_val = np.array(X_val)\n",
    "# Y_val = np.array(Y_val)\n",
    "X_test = np.array(X_test)\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f27e038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5798178, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6027a35e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5798178, 150)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4cd3acd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dividing by the maximum value for making values 0 - 1\n",
    "Y_train[:, 0] = Y_train[:,0]  /  70.3026558891455"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21a7bd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test[:,0] = Y_test[:,0] / 70.3026558891455\n",
    "Y_train[:, 1] = Y_train[:,1]  /  227.9885103926097\n",
    "Y_test[:, 1] = Y_test[:,1]  /  227.9885103926097\n",
    "Y_train[:, 2] = Y_train[:,2]  /  159.28123556581986\n",
    "Y_test[:, 2] = Y_test[:,2]  /  159.28123556581986\n",
    "Y_train[:, 3] = Y_train[:,3]  /  6.782719399538106\n",
    "Y_test[:, 3] = Y_test[:,3]  /  6.782719399538106"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e3cd689",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-22 08:27:00.983441: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-22 08:27:01.030170: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-22 08:27:01.276551: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-22 08:27:01.277860: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-22 08:27:02.286181: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import (Concatenate, Conv1D, Dense, Flatten,\n",
    "                                     Input, MaxPooling1D, Reshape)\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "def getKerasModel(model_name):\n",
    "    \"\"\"Get keras model by name.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_name : str\n",
    "        Name of the respective model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Sequential keras model\n",
    "        Model.\n",
    "\n",
    "    \"\"\"\n",
    "    if model_name == \"LucasCNN\":\n",
    "        return LucasCNN()\n",
    "    if model_name == \"HuEtAl\":\n",
    "        return HuEtAl()\n",
    "    if model_name == \"LiuEtAl\":\n",
    "        return LiuEtAl()\n",
    "    if model_name == \"LucasResNet\":\n",
    "        return LucasResNet()\n",
    "    if model_name == \"LucasCoordConv\":\n",
    "        return LucasCoordConv()\n",
    "    print(\"Error: Model {0} not implemented.\".format(model_name))\n",
    "    return None\n",
    "\n",
    "\n",
    "def HuEtAl():\n",
    "    \"\"\"Return 1D-CNN by Wei Hu et al 2014.\"\"\"\n",
    "    seq_length = 150\n",
    "\n",
    "    # definition by Hu et al for parameter k1 and k2\n",
    "    kernel_size = seq_length // 9\n",
    "    pool_size = int((seq_length - kernel_size + 1) / 35)\n",
    "\n",
    "    inp = Input(shape=(seq_length, 1))\n",
    "\n",
    "    # CONV1\n",
    "    x = Conv1D(filters=20, kernel_size=kernel_size, activation=\"tanh\")(inp)\n",
    "    x = MaxPooling1D(pool_size)(x)\n",
    "\n",
    "    # Flatten, FC1, Softmax\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(units=100, activation=\"tanh\")(x)\n",
    "    x = Dense(4, activation=\"linear\")(x)\n",
    "\n",
    "    return Model(inputs=inp, outputs=x)\n",
    "\n",
    "\n",
    "def LiuEtAl():\n",
    "    \"\"\"Return 1D-CNN by Lanfa Liu et al 2018.\"\"\"\n",
    "    seq_length = 150\n",
    "    kernel_size = 3\n",
    "\n",
    "    inp = Input(shape=(seq_length, 1))\n",
    "\n",
    "    # CONV1\n",
    "    x = Conv1D(filters=32, kernel_size=kernel_size, activation=\"relu\")(inp)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "\n",
    "    # CONV2\n",
    "    x = Conv1D(filters=32, kernel_size=kernel_size, activation='relu')(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "\n",
    "    # CONV3\n",
    "    x = Conv1D(filters=64, kernel_size=kernel_size, activation='relu')(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "\n",
    "    # CONV4\n",
    "    x = Conv1D(filters=64, kernel_size=kernel_size, activation='relu')(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "\n",
    "    # Flatten & Softmax\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(4, activation=\"linear\")(x)\n",
    "\n",
    "    return Model(inputs=inp, outputs=x)\n",
    "\n",
    "\n",
    "def LucasCNN():\n",
    "    \"\"\"Return LucasCNN implementation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Sequential keras model\n",
    "        Model.\n",
    "\n",
    "    \"\"\"\n",
    "    seq_length = 150\n",
    "    kernel_size = 3\n",
    "    activation = \"relu\"\n",
    "    padding = \"valid\"\n",
    "\n",
    "    inp = Input(shape=(seq_length, 1))\n",
    "\n",
    "    # CONV1\n",
    "    x = Conv1D(filters=32,\n",
    "               kernel_size=kernel_size,\n",
    "               activation=activation,\n",
    "               padding=padding)(inp)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "\n",
    "    # CONV2\n",
    "    x = Conv1D(filters=32,\n",
    "               kernel_size=kernel_size,\n",
    "               activation=activation,\n",
    "               padding=padding)(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "\n",
    "    # CONV3\n",
    "    x = Conv1D(filters=64,\n",
    "               kernel_size=kernel_size,\n",
    "               activation=activation,\n",
    "               padding=padding)(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "\n",
    "    # CONV4\n",
    "    x = Conv1D(filters=64,\n",
    "               kernel_size=kernel_size,\n",
    "               activation=activation,\n",
    "               padding=padding)(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "\n",
    "    # Flatten, FC1, FC2, Softmax\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(120, activation=activation)(x)\n",
    "    x = Dense(160, activation=activation)(x)\n",
    "    x = Dense(4, activation=\"linear\")(x)\n",
    "\n",
    "    return Model(inputs=inp, outputs=x)\n",
    "\n",
    "\n",
    "def LucasResNet():\n",
    "    \"\"\"Return LucasResNet implementation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Sequential keras model\n",
    "        Model.\n",
    "\n",
    "    \"\"\"\n",
    "    seq_length = 150\n",
    "    kernel_size = 3\n",
    "    activation = \"relu\"\n",
    "    padding = \"same\"\n",
    "\n",
    "    inp = Input(shape=(seq_length, 1))\n",
    "\n",
    "    # CONV1\n",
    "    x = Conv1D(filters=32,\n",
    "               kernel_size=kernel_size,\n",
    "               activation=activation,\n",
    "               padding=padding)(inp)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "\n",
    "    # CONV2\n",
    "    x = Conv1D(filters=32,\n",
    "               kernel_size=kernel_size,\n",
    "               activation=activation,\n",
    "               padding=padding)(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "\n",
    "    # CONV3\n",
    "    x = Conv1D(filters=64,\n",
    "               kernel_size=kernel_size,\n",
    "               activation=activation,\n",
    "               padding=padding)(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "\n",
    "    # CONV4\n",
    "    x = Conv1D(filters=64,\n",
    "               kernel_size=kernel_size,\n",
    "               activation=activation,\n",
    "               padding=padding)(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "\n",
    "    # Residual block\n",
    "    inp_res = Reshape((10, 15))(inp)\n",
    "    x = Concatenate(axis=-1)([x, inp_res])\n",
    "\n",
    "    # Flatten, FC1, FC2, Softmax\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(150, activation=activation)(x)\n",
    "    x = Dense(100, activation=activation)(x)\n",
    "    x = Dense(4, activation=\"linear\")(x)\n",
    "\n",
    "    return Model(inputs=inp, outputs=x)\n",
    "\n",
    "\n",
    "def LucasCoordConv():\n",
    "    \"\"\"Return LucasCoordConv implementation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Sequential keras model\n",
    "        Model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    seq_length = 150\n",
    "    kernel_size = 3\n",
    "    activation = \"relu\"\n",
    "    padding = \"valid\"\n",
    "\n",
    "    inp = Input(shape=(seq_length, 1))\n",
    "\n",
    "    # CoordCONV1\n",
    "    x = Conv1D(filters=32,\n",
    "               kernel_size=kernel_size,\n",
    "               activation=activation,\n",
    "               padding=padding)(inp)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "\n",
    "    # CONV2\n",
    "    x = Conv1D(filters=64,\n",
    "               kernel_size=kernel_size,\n",
    "               activation=activation,\n",
    "               padding=padding)(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "\n",
    "    # CONV3\n",
    "    x = Conv1D(filters=64,\n",
    "               kernel_size=kernel_size,\n",
    "               activation=activation,\n",
    "               padding=padding)(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "\n",
    "    # CONV4\n",
    "    x = Conv1D(filters=128,\n",
    "               kernel_size=kernel_size,\n",
    "               activation=activation,\n",
    "               padding=padding)(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "\n",
    "    # Flatte, FC1, FC2, Softmax\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(256, activation=activation)(x)\n",
    "    x = Dense(128, activation=activation)(x)\n",
    "    x = Dense(4, activation=\"linear\")(x)\n",
    "\n",
    "    return Model(inputs=inp, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4065686b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Instantiate the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "# X_val = scaler.transform(X_val)\n",
    "# X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "228b3e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "18120/18120 [==============================] - 203s 11ms/step - loss: 0.0447 - mae: 0.1384 - val_loss: 0.0800 - val_mae: 0.1814\n",
      "Epoch 2/15\n",
      "18120/18120 [==============================] - 202s 11ms/step - loss: 0.0389 - mae: 0.1273 - val_loss: 0.0806 - val_mae: 0.1817\n",
      "Epoch 3/15\n",
      "18120/18120 [==============================] - 230s 13ms/step - loss: 0.0374 - mae: 0.1242 - val_loss: 0.0808 - val_mae: 0.1858\n",
      "Epoch 4/15\n",
      "18120/18120 [==============================] - 202s 11ms/step - loss: 0.0366 - mae: 0.1225 - val_loss: 0.0820 - val_mae: 0.1846\n",
      "Epoch 5/15\n",
      "18120/18120 [==============================] - 205s 11ms/step - loss: 0.0360 - mae: 0.1214 - val_loss: 0.0820 - val_mae: 0.1826\n",
      "Epoch 6/15\n",
      "18120/18120 [==============================] - 205s 11ms/step - loss: 0.0356 - mae: 0.1206 - val_loss: 0.0809 - val_mae: 0.1859\n",
      "Epoch 7/15\n",
      "18120/18120 [==============================] - 205s 11ms/step - loss: 0.0353 - mae: 0.1200 - val_loss: 0.0830 - val_mae: 0.1852\n",
      "Epoch 8/15\n",
      "18120/18120 [==============================] - 205s 11ms/step - loss: 0.0351 - mae: 0.1196 - val_loss: 0.0826 - val_mae: 0.1867\n",
      "Epoch 9/15\n",
      "18120/18120 [==============================] - 205s 11ms/step - loss: 0.0349 - mae: 0.1192 - val_loss: 0.0815 - val_mae: 0.1832\n",
      "Epoch 10/15\n",
      "18120/18120 [==============================] - 205s 11ms/step - loss: 0.0347 - mae: 0.1189 - val_loss: 0.0832 - val_mae: 0.1884\n",
      "Epoch 11/15\n",
      "18120/18120 [==============================] - 205s 11ms/step - loss: 0.0346 - mae: 0.1185 - val_loss: 0.0820 - val_mae: 0.1869\n",
      "Epoch 12/15\n",
      "18120/18120 [==============================] - 205s 11ms/step - loss: 0.0344 - mae: 0.1183 - val_loss: 0.0830 - val_mae: 0.1886\n",
      "Epoch 13/15\n",
      "18120/18120 [==============================] - 205s 11ms/step - loss: 0.0343 - mae: 0.1181 - val_loss: 0.0829 - val_mae: 0.1871\n",
      "Epoch 14/15\n",
      "18120/18120 [==============================] - 205s 11ms/step - loss: 0.0342 - mae: 0.1179 - val_loss: 0.0820 - val_mae: 0.1848\n",
      "Epoch 15/15\n",
      "18120/18120 [==============================] - 205s 11ms/step - loss: 0.0341 - mae: 0.1177 - val_loss: 0.0805 - val_mae: 0.1828\n",
      "36239/36239 [==============================] - 138s 4ms/step - loss: 0.0805 - mae: 0.1828\n",
      "Test Loss: 0.08052553981542587\n",
      "Test MAE: 0.18282243609428406\n",
      "Epoch 1/15\n",
      "18120/18120 [==============================] - 208s 11ms/step - loss: 0.0459 - mae: 0.1394 - val_loss: 0.0802 - val_mae: 0.1825\n",
      "Epoch 2/15\n",
      "18120/18120 [==============================] - 207s 11ms/step - loss: 0.0406 - mae: 0.1290 - val_loss: 0.0783 - val_mae: 0.1802\n",
      "Epoch 3/15\n",
      "18120/18120 [==============================] - 207s 11ms/step - loss: 0.0392 - mae: 0.1262 - val_loss: 0.0799 - val_mae: 0.1856\n",
      "Epoch 4/15\n",
      "18120/18120 [==============================] - 208s 11ms/step - loss: 0.0384 - mae: 0.1247 - val_loss: 0.0796 - val_mae: 0.1824\n",
      "Epoch 5/15\n",
      "18120/18120 [==============================] - 207s 11ms/step - loss: 0.0378 - mae: 0.1236 - val_loss: 0.0797 - val_mae: 0.1814\n",
      "Epoch 6/15\n",
      "18120/18120 [==============================] - 207s 11ms/step - loss: 0.0375 - mae: 0.1228 - val_loss: 0.0805 - val_mae: 0.1829\n",
      "Epoch 7/15\n",
      "18120/18120 [==============================] - 207s 11ms/step - loss: 0.0372 - mae: 0.1222 - val_loss: 0.0823 - val_mae: 0.1840\n",
      "Epoch 8/15\n",
      "18120/18120 [==============================] - 203s 11ms/step - loss: 0.0369 - mae: 0.1217 - val_loss: 0.0806 - val_mae: 0.1831\n",
      "Epoch 9/15\n",
      "18120/18120 [==============================] - 197s 11ms/step - loss: 0.0367 - mae: 0.1213 - val_loss: 0.0813 - val_mae: 0.1844\n",
      "Epoch 10/15\n",
      "18120/18120 [==============================] - 200s 11ms/step - loss: 0.0366 - mae: 0.1210 - val_loss: 0.0810 - val_mae: 0.1843\n",
      "Epoch 11/15\n",
      "18120/18120 [==============================] - 199s 11ms/step - loss: 0.0364 - mae: 0.1207 - val_loss: 0.0817 - val_mae: 0.1826\n",
      "Epoch 12/15\n",
      "18120/18120 [==============================] - 199s 11ms/step - loss: 0.0363 - mae: 0.1205 - val_loss: 0.0837 - val_mae: 0.1870\n",
      "Epoch 13/15\n",
      "18120/18120 [==============================] - 198s 11ms/step - loss: 0.0362 - mae: 0.1203 - val_loss: 0.0804 - val_mae: 0.1821\n",
      "Epoch 14/15\n",
      "18120/18120 [==============================] - 199s 11ms/step - loss: 0.0361 - mae: 0.1202 - val_loss: 0.0812 - val_mae: 0.1825\n",
      "Epoch 15/15\n",
      "18120/18120 [==============================] - 198s 11ms/step - loss: 0.0360 - mae: 0.1200 - val_loss: 0.0832 - val_mae: 0.1871\n",
      "36239/36239 [==============================] - 104s 3ms/step - loss: 0.0832 - mae: 0.1871\n",
      "Test Loss: 0.08317136019468307\n",
      "Test MAE: 0.1871417611837387\n",
      "Epoch 1/15\n",
      "18120/18120 [==============================] - 203s 11ms/step - loss: 0.0501 - mae: 0.1439 - val_loss: 0.0589 - val_mae: 0.1576\n",
      "Epoch 2/15\n",
      "18120/18120 [==============================] - 202s 11ms/step - loss: 0.0442 - mae: 0.1335 - val_loss: 0.0597 - val_mae: 0.1576\n",
      "Epoch 3/15\n",
      "18120/18120 [==============================] - 202s 11ms/step - loss: 0.0426 - mae: 0.1305 - val_loss: 0.0572 - val_mae: 0.1560\n",
      "Epoch 4/15\n",
      "18120/18120 [==============================] - 205s 11ms/step - loss: 0.0417 - mae: 0.1288 - val_loss: 0.0616 - val_mae: 0.1609\n",
      "Epoch 5/15\n",
      "18120/18120 [==============================] - 205s 11ms/step - loss: 0.0411 - mae: 0.1276 - val_loss: 0.0587 - val_mae: 0.1588\n",
      "Epoch 6/15\n",
      "18120/18120 [==============================] - 202s 11ms/step - loss: 0.0407 - mae: 0.1269 - val_loss: 0.0591 - val_mae: 0.1569\n",
      "Epoch 7/15\n",
      "18120/18120 [==============================] - 206s 11ms/step - loss: 0.0403 - mae: 0.1264 - val_loss: 0.0575 - val_mae: 0.1559\n",
      "Epoch 8/15\n",
      "18120/18120 [==============================] - 205s 11ms/step - loss: 0.0401 - mae: 0.1259 - val_loss: 0.0610 - val_mae: 0.1603\n",
      "Epoch 9/15\n",
      "18120/18120 [==============================] - 193s 11ms/step - loss: 0.0398 - mae: 0.1255 - val_loss: 0.0608 - val_mae: 0.1597\n",
      "Epoch 10/15\n",
      "18120/18120 [==============================] - 190s 10ms/step - loss: 0.0397 - mae: 0.1252 - val_loss: 0.0599 - val_mae: 0.1589\n",
      "Epoch 11/15\n",
      "18120/18120 [==============================] - 186s 10ms/step - loss: 0.0395 - mae: 0.1250 - val_loss: 0.0575 - val_mae: 0.1560\n",
      "Epoch 12/15\n",
      "18120/18120 [==============================] - 185s 10ms/step - loss: 0.0394 - mae: 0.1247 - val_loss: 0.0630 - val_mae: 0.1618\n",
      "Epoch 13/15\n",
      "18120/18120 [==============================] - 202s 11ms/step - loss: 0.0393 - mae: 0.1246 - val_loss: 0.0589 - val_mae: 0.1574\n",
      "Epoch 14/15\n",
      "18120/18120 [==============================] - 206s 11ms/step - loss: 0.0392 - mae: 0.1244 - val_loss: 0.0595 - val_mae: 0.1573\n",
      "Epoch 15/15\n",
      "18120/18120 [==============================] - 207s 11ms/step - loss: 0.0391 - mae: 0.1242 - val_loss: 0.0593 - val_mae: 0.1576\n",
      "36239/36239 [==============================] - 142s 4ms/step - loss: 0.0593 - mae: 0.1576\n",
      "Test Loss: 0.05930153653025627\n",
      "Test MAE: 0.15759696066379547\n",
      "Epoch 1/15\n",
      "18120/18120 [==============================] - 209s 11ms/step - loss: 0.0533 - mae: 0.1493 - val_loss: 0.0594 - val_mae: 0.1781\n",
      "Epoch 2/15\n",
      "18120/18120 [==============================] - 206s 11ms/step - loss: 0.0469 - mae: 0.1380 - val_loss: 0.0560 - val_mae: 0.1706\n",
      "Epoch 3/15\n",
      "18120/18120 [==============================] - 206s 11ms/step - loss: 0.0451 - mae: 0.1350 - val_loss: 0.0601 - val_mae: 0.1767\n",
      "Epoch 4/15\n",
      "18120/18120 [==============================] - 208s 11ms/step - loss: 0.0442 - mae: 0.1334 - val_loss: 0.0586 - val_mae: 0.1736\n",
      "Epoch 5/15\n",
      "18120/18120 [==============================] - 210s 12ms/step - loss: 0.0436 - mae: 0.1323 - val_loss: 0.0627 - val_mae: 0.1804\n",
      "Epoch 6/15\n",
      "18120/18120 [==============================] - 210s 12ms/step - loss: 0.0432 - mae: 0.1316 - val_loss: 0.0634 - val_mae: 0.1790\n",
      "Epoch 7/15\n",
      "18120/18120 [==============================] - 208s 12ms/step - loss: 0.0428 - mae: 0.1310 - val_loss: 0.0572 - val_mae: 0.1721\n",
      "Epoch 8/15\n",
      "18120/18120 [==============================] - 209s 12ms/step - loss: 0.0426 - mae: 0.1305 - val_loss: 0.0616 - val_mae: 0.1766\n",
      "Epoch 9/15\n",
      "18120/18120 [==============================] - 211s 12ms/step - loss: 0.0423 - mae: 0.1302 - val_loss: 0.0579 - val_mae: 0.1729\n",
      "Epoch 10/15\n",
      "18120/18120 [==============================] - 207s 11ms/step - loss: 0.0421 - mae: 0.1298 - val_loss: 0.0564 - val_mae: 0.1710\n",
      "Epoch 11/15\n",
      "18120/18120 [==============================] - 205s 11ms/step - loss: 0.0420 - mae: 0.1295 - val_loss: 0.0570 - val_mae: 0.1719\n",
      "Epoch 12/15\n",
      "18120/18120 [==============================] - 198s 11ms/step - loss: 0.0418 - mae: 0.1294 - val_loss: 0.0614 - val_mae: 0.1762\n",
      "Epoch 13/15\n",
      "18120/18120 [==============================] - 207s 11ms/step - loss: 0.0417 - mae: 0.1291 - val_loss: 0.0581 - val_mae: 0.1708\n",
      "Epoch 14/15\n",
      "18120/18120 [==============================] - 198s 11ms/step - loss: 0.0416 - mae: 0.1290 - val_loss: 0.0582 - val_mae: 0.1721\n",
      "Epoch 15/15\n",
      "18120/18120 [==============================] - 193s 11ms/step - loss: 0.0415 - mae: 0.1288 - val_loss: 0.0564 - val_mae: 0.1699\n",
      "36239/36239 [==============================] - 100s 3ms/step - loss: 0.0564 - mae: 0.1699\n",
      "Test Loss: 0.05635342374444008\n",
      "Test MAE: 0.1698915660381317\n",
      "Epoch 1/15\n",
      "18120/18120 [==============================] - 195s 11ms/step - loss: 0.0488 - mae: 0.1441 - val_loss: 0.0761 - val_mae: 0.1895\n",
      "Epoch 2/15\n",
      "18120/18120 [==============================] - 194s 11ms/step - loss: 0.0422 - mae: 0.1323 - val_loss: 0.0837 - val_mae: 0.1947\n",
      "Epoch 3/15\n",
      "18120/18120 [==============================] - 194s 11ms/step - loss: 0.0404 - mae: 0.1288 - val_loss: 0.0854 - val_mae: 0.1960\n",
      "Epoch 4/15\n",
      "18120/18120 [==============================] - 194s 11ms/step - loss: 0.0395 - mae: 0.1271 - val_loss: 0.0813 - val_mae: 0.1911\n",
      "Epoch 5/15\n",
      "18120/18120 [==============================] - 194s 11ms/step - loss: 0.0389 - mae: 0.1259 - val_loss: 0.0864 - val_mae: 0.1969\n",
      "Epoch 6/15\n",
      "18120/18120 [==============================] - 194s 11ms/step - loss: 0.0385 - mae: 0.1251 - val_loss: 0.0818 - val_mae: 0.1919\n",
      "Epoch 7/15\n",
      "18120/18120 [==============================] - 194s 11ms/step - loss: 0.0381 - mae: 0.1245 - val_loss: 0.0736 - val_mae: 0.1853\n",
      "Epoch 8/15\n",
      "18120/18120 [==============================] - 194s 11ms/step - loss: 0.0379 - mae: 0.1240 - val_loss: 0.0784 - val_mae: 0.1886\n",
      "Epoch 9/15\n",
      "18120/18120 [==============================] - 194s 11ms/step - loss: 0.0377 - mae: 0.1236 - val_loss: 0.0777 - val_mae: 0.1865\n",
      "Epoch 10/15\n",
      "18120/18120 [==============================] - 194s 11ms/step - loss: 0.0375 - mae: 0.1233 - val_loss: 0.0984 - val_mae: 0.2098\n",
      "Epoch 11/15\n",
      "18120/18120 [==============================] - 194s 11ms/step - loss: 0.0373 - mae: 0.1230 - val_loss: 0.0955 - val_mae: 0.2068\n",
      "Epoch 12/15\n",
      "18120/18120 [==============================] - 194s 11ms/step - loss: 0.0371 - mae: 0.1227 - val_loss: 0.0874 - val_mae: 0.1991\n",
      "Epoch 13/15\n",
      "18120/18120 [==============================] - 207s 11ms/step - loss: 0.0370 - mae: 0.1225 - val_loss: 0.0902 - val_mae: 0.2037\n",
      "Epoch 14/15\n",
      "18120/18120 [==============================] - 208s 11ms/step - loss: 0.0369 - mae: 0.1223 - val_loss: 0.0854 - val_mae: 0.1967\n",
      "Epoch 15/15\n",
      "18120/18120 [==============================] - 208s 11ms/step - loss: 0.0368 - mae: 0.1221 - val_loss: 0.0850 - val_mae: 0.1974\n",
      "36239/36239 [==============================] - 142s 4ms/step - loss: 0.0850 - mae: 0.1974\n",
      "Test Loss: 0.08502476662397385\n",
      "Test MAE: 0.19735941290855408\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Assuming X_train, Y_train, X_test, y_test are defined\n",
    "\n",
    "k_folds = 5\n",
    "\n",
    "def train_regression_model(model, X_train, Y_train, X_test, y_test, epochs=15, batch_size=256):\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test))\n",
    "    \n",
    "    # Evaluate the model\n",
    "    loss, mae = model.evaluate(X_test, y_test)\n",
    "    print(\"Test Loss:\", loss)\n",
    "    print(\"Test MAE:\", mae)\n",
    "    \n",
    "    return model, loss\n",
    "\n",
    "best_model = None\n",
    "best_rmse = 10000\n",
    "# Initialize a KFold with the number of splits\n",
    "n_splits = 5  # You can adjust this as needed\n",
    "kfold = KFold(n_splits=n_splits)\n",
    "results = []\n",
    "for train_idx, test_idx in kfold.split(X_train):\n",
    "    X_train1, y_train1 = X_train[train_idx], Y_train[train_idx]\n",
    "    X_test1, y_test1 = X_train[test_idx], Y_train[test_idx]\n",
    "    \n",
    "    model = getKerasModel(\"LucasCoordConv\")\n",
    "\n",
    "    model, rmse = train_regression_model(model, X_train1, y_train1, X_test1, y_test1)\n",
    "    results.append(rmse)\n",
    "\n",
    "    # Check if this model has a lower RMSE than the best model so far\n",
    "    if rmse < best_rmse:\n",
    "        best_model = model\n",
    "        best_rmse = rmse\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e482dfa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "18123/18123 [==============================] - 210s 12ms/step - loss: 0.0381 - mae: 0.1242 - val_loss: 0.0436 - val_mae: 0.1318\n",
      "Epoch 2/15\n",
      "18123/18123 [==============================] - 207s 11ms/step - loss: 0.0380 - mae: 0.1241 - val_loss: 0.0432 - val_mae: 0.1314\n",
      "Epoch 3/15\n",
      "14074/18123 [======================>.......] - ETA: 40s - loss: 0.0379 - mae: 0.1240"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=15, batch_size=256, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fd2e464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_regression_model(model, X_train, Y_train, X_test, y_test, epochs=15, batch_size=256):\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test))\n",
    "    \n",
    "    # Evaluate the model\n",
    "    loss, mae = model.evaluate(X_test, y_test)\n",
    "    print(\"Test Loss:\", loss)\n",
    "    print(\"Test MAE:\", mae)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate Mean Squared Error\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(\"root Mean Squared Error:\", mse ** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7977c151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_2 (Conv1D)           (None, 148, 64)           256       \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 74, 64)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 72, 64)            12352     \n",
      "                                                                 \n",
      " max_pooling1d_3 (MaxPooling  (None, 36, 64)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                147520    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4)                 260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160,388\n",
      "Trainable params: 160,388\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "\n",
    "# Define the input shape\n",
    "input_shape = (150, 1)  # Assuming your input data has 100 features\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a 1D convolutional layer\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "\n",
    "# Add a max pooling layer\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# Add another 1D convolutional layer\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "\n",
    "# Add a max pooling layer\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# Flatten the output\n",
    "model.add(Flatten())\n",
    "\n",
    "# model.add(Dense(256, activation='relu'))\n",
    "\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# Add a dense layer\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Add the output layer\n",
    "model.add(Dense(4))  # Assuming you're doing regression, so no activation function\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12efdf25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "18123/18123 [==============================] - 144s 8ms/step - loss: 0.0081 - mae: 0.0670 - val_loss: 0.0077 - val_mae: 0.0650\n",
      "Epoch 2/5\n",
      "18123/18123 [==============================] - 141s 8ms/step - loss: 0.0074 - mae: 0.0637 - val_loss: 0.0075 - val_mae: 0.0639\n",
      "Epoch 3/5\n",
      "18123/18123 [==============================] - 139s 8ms/step - loss: 0.0072 - mae: 0.0628 - val_loss: 0.0074 - val_mae: 0.0638\n",
      "Epoch 4/5\n",
      "18123/18123 [==============================] - 140s 8ms/step - loss: 0.0071 - mae: 0.0622 - val_loss: 0.0073 - val_mae: 0.0631\n",
      "Epoch 5/5\n",
      "18123/18123 [==============================] - 142s 8ms/step - loss: 0.0070 - mae: 0.0619 - val_loss: 0.0072 - val_mae: 0.0625\n",
      "36215/36215 [==============================] - 104s 3ms/step - loss: 0.0072 - mae: 0.0625\n",
      "Test Loss: 0.00723271956667304\n",
      "Test MAE: 0.062470924109220505\n",
      "36215/36215 [==============================] - 95s 3ms/step\n",
      "root Mean Squared Error: 0.0850453066638279\n"
     ]
    }
   ],
   "source": [
    "train_regression_model(model, X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e243ab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "# Instantiate the Random Forest model (for classification)\n",
    "# Change RandomForestClassifier to RandomForestRegressor for regression tasks\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mse = mean_squared_error(Y_train, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1e83b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.reshape(Y_train[:,0], 4639319)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bcf629d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4639319,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3b15d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 02:21:39.281954: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-07 02:21:39.333691: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-07 02:21:39.581026: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-07 02:21:39.582428: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-07 02:21:40.567096: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-03-07 02:21:44.656025: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Assuming X_train is your input data and y_train is your regression targets\n",
    "\n",
    "# Convert numerical data into text tokens\n",
    "X_train_diff = np.diff(X_train)\n",
    "\n",
    "X_train_diff = np.maximum(0, X_train_diff)\n",
    "\n",
    "# Example: converting a numerical vector [1, 2, 3, ..., 150] into a space-separated string '1 2 3 ... 150'\n",
    "X_train_text = [' '.join(map(str, vec)) for vec in X_train_diff]\n",
    "\n",
    "# Tokenize input\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for text in X_train_text:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        text,\n",
    "                        add_special_tokens = True,\n",
    "                        max_length = 149,  # Assuming max length is 150\n",
    "                        padding = 'max_length',\n",
    "                        truncation = True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'tf'\n",
    "                   )\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d21bd559",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2621/14498 [====>.........................] - ETA: 79:54:50 - loss: 2138.3713 - mean_absolute_error: 26.5699"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_absolute_error\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_masks\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_ids = tf.concat(input_ids, axis=0)\n",
    "attention_masks = tf.concat(attention_masks, axis=0)\n",
    "\n",
    "# Define model architecture\n",
    "input_layer = Input(shape=(149,), dtype=tf.int32)\n",
    "mask_layer = Input(shape=(149,), dtype=tf.int32)\n",
    "\n",
    "bert_output = bert_model(input_layer, attention_mask=mask_layer)[0]\n",
    "output = Dense(4)(bert_output[:, 0, :])  # No activation function for regression\n",
    "\n",
    "model = Model(inputs=[input_layer, mask_layer], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(lr=2e-5)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Train the model\n",
    "model.fit([input_ids, attention_masks], Y_train, epochs=5, batch_size=256, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b8c85c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 498,  500,  504,  508,  517,  542,  552,  552,  554,  562,  573,\n",
       "        587,  593,  612,  633,  641,  650,  670,  681,  682,  688,  694,\n",
       "        722,  749,  764,  777,  796,  808,  826,  844,  866,  882,  893,\n",
       "        902,  911,  915,  927,  940,  957,  972,  987,  994, 1000, 1005,\n",
       "       1018, 1032, 1044, 1046, 1050, 1056, 1065, 1069, 1072, 1072, 1077,\n",
       "       1084, 1091, 1112, 1127, 1127, 1128, 1129, 1130, 1131, 1132, 1133,\n",
       "       1134, 1141, 1148, 1188, 1233, 1273, 1302, 1319, 1320, 1323, 1327,\n",
       "       1334, 1341, 1348, 1356, 1363, 1370, 1376, 1384, 1392, 1400, 1407,\n",
       "       1412, 1416, 1418, 1419, 1422, 1424, 1425, 1429, 1433, 1455, 1479,\n",
       "       1498, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1514, 1515,\n",
       "       1521, 1526, 1531, 1536, 1543, 1546, 1548, 1554, 1562, 1585, 1607,\n",
       "       1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1627,\n",
       "       1631, 1638, 1639, 1641, 1643, 1646, 1651, 1652, 1652, 1652, 1655,\n",
       "       1657, 1659, 1662, 1665, 1668, 1671, 1674], dtype=int16)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bb0dc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_array(arr):\n",
    "    n, m = arr.shape\n",
    "    for i in range(n):\n",
    "        for j in range(m - 1):\n",
    "            if arr[i, j + 1] < arr[i, j]:\n",
    "                if j + 2 < m and arr[i, j + 2] >= arr[i, j]:\n",
    "                    arr[i, j + 1] = (arr[i, j] + arr[i, j + 2]) / 2\n",
    "                else:\n",
    "                    arr[i, j + 1] = arr[i, j] + 1\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d71f123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# judge that in ends we have better results means the error is more due to the 11 x 11 patch think more about them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d218134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diff(X_train[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f534f569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b88dcc864714555afb136bd44ebcc22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading training data ..:   0%|          | 0/1154 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 6ms/step\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "3/3 [==============================] - 0s 6ms/step\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "3/3 [==============================] - 0s 6ms/step\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 6ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 6ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 3ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 4ms/step\n",
      "3/3 [==============================] - 0s 5ms/step\n",
      "27/27 [==============================] - 0s 3ms/step\n",
      "31/31 [==============================] - 0s 3ms/step\n",
      "21/21 [==============================] - 0s 3ms/step\n",
      "29/29 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "36/36 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 2ms/step\n",
      "36/36 [==============================] - 0s 2ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "36/36 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "10/10 [==============================] - 0s 3ms/step\n",
      "36/36 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "36/36 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "36/36 [==============================] - 0s 3ms/step\n",
      "36/36 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "36/36 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "30/30 [==============================] - 0s 4ms/step\n",
      "36/36 [==============================] - 0s 3ms/step\n",
      "36/36 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "36/36 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "36/36 [==============================] - 0s 3ms/step\n",
      "27/27 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "24/24 [==============================] - 0s 3ms/step\n",
      "30/30 [==============================] - 0s 3ms/step\n",
      "28/28 [==============================] - 0s 3ms/step\n",
      "28/28 [==============================] - 0s 3ms/step\n",
      "27/27 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "28/28 [==============================] - 0s 3ms/step\n",
      "34/34 [==============================] - 0s 3ms/step\n",
      "29/29 [==============================] - 0s 3ms/step\n",
      "31/31 [==============================] - 0s 3ms/step\n",
      "36/36 [==============================] - 0s 3ms/step\n",
      "36/36 [==============================] - 0s 3ms/step\n",
      "26/26 [==============================] - 0s 3ms/step\n",
      "12/12 [==============================] - 0s 3ms/step\n",
      "22/22 [==============================] - 0s 3ms/step\n",
      "26/26 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "34/34 [==============================] - 0s 3ms/step\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "28/28 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "39/39 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "31/31 [==============================] - 0s 3ms/step\n",
      "37/37 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "34/34 [==============================] - 0s 3ms/step\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "28/28 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "31/31 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "22/22 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "28/28 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "21/21 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "34/34 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "26/26 [==============================] - 0s 4ms/step\n",
      "27/27 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "31/31 [==============================] - 0s 4ms/step\n",
      "34/34 [==============================] - 0s 3ms/step\n",
      "39/39 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "40/40 [==============================] - 0s 4ms/step\n",
      "36/36 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "23/23 [==============================] - 0s 4ms/step\n",
      "24/24 [==============================] - 0s 3ms/step\n",
      "27/27 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "37/37 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "46/46 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "44/44 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "37/37 [==============================] - 0s 3ms/step\n",
      "34/34 [==============================] - 0s 4ms/step\n",
      "25/25 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "37/37 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "36/36 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "36/36 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "26/26 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "37/37 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "47/47 [==============================] - 0s 3ms/step\n",
      "41/41 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "40/40 [==============================] - 0s 4ms/step\n",
      "40/40 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "41/41 [==============================] - 0s 3ms/step\n",
      "32/32 [==============================] - 0s 4ms/step\n",
      "23/23 [==============================] - 0s 4ms/step\n",
      "37/37 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "23/23 [==============================] - 0s 3ms/step\n",
      "25/25 [==============================] - 0s 4ms/step\n",
      "29/29 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "37/37 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "34/34 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "36/36 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "36/36 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 4ms/step\n",
      "29/29 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "48/48 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "35/35 [==============================] - 0s 3ms/step\n",
      "34/34 [==============================] - 0s 4ms/step\n",
      "39/39 [==============================] - 0s 3ms/step\n",
      "42/42 [==============================] - 0s 4ms/step\n",
      "43/43 [==============================] - 0s 3ms/step\n",
      "19/19 [==============================] - 0s 3ms/step\n",
      "234/234 [==============================] - 1s 4ms/step\n",
      "213/213 [==============================] - 1s 4ms/step\n",
      "248/248 [==============================] - 1s 4ms/step\n",
      "249/249 [==============================] - 1s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "320/320 [==============================] - 1s 3ms/step\n",
      "236/236 [==============================] - 1s 4ms/step\n",
      "262/262 [==============================] - 1s 3ms/step\n",
      "150/150 [==============================] - 1s 4ms/step\n",
      "261/261 [==============================] - 1s 4ms/step\n",
      "216/216 [==============================] - 1s 4ms/step\n",
      "209/209 [==============================] - 1s 4ms/step\n",
      "135/135 [==============================] - 0s 3ms/step\n",
      "279/279 [==============================] - 1s 4ms/step\n",
      "126/126 [==============================] - 0s 3ms/step\n",
      "265/265 [==============================] - 1s 4ms/step\n",
      "157/157 [==============================] - 1s 4ms/step\n",
      "281/281 [==============================] - 1s 4ms/step\n",
      "321/321 [==============================] - 1s 4ms/step\n",
      "237/237 [==============================] - 1s 4ms/step\n",
      "148/148 [==============================] - 1s 4ms/step\n",
      "114/114 [==============================] - 0s 3ms/step\n",
      "210/210 [==============================] - 1s 3ms/step\n",
      "172/172 [==============================] - 1s 4ms/step\n",
      "229/229 [==============================] - 1s 4ms/step\n",
      "237/237 [==============================] - 1s 4ms/step\n",
      "85/85 [==============================] - 0s 3ms/step\n",
      "341/341 [==============================] - 1s 4ms/step\n",
      "170/170 [==============================] - 1s 4ms/step\n",
      "177/177 [==============================] - 1s 4ms/step\n",
      "266/266 [==============================] - 1s 4ms/step\n",
      "280/280 [==============================] - 1s 4ms/step\n",
      "271/271 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "318/318 [==============================] - 1s 4ms/step\n",
      "118/118 [==============================] - 0s 4ms/step\n",
      "93/93 [==============================] - 0s 4ms/step\n",
      "304/304 [==============================] - 1s 4ms/step\n",
      "144/144 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "239/239 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "205/205 [==============================] - 1s 4ms/step\n",
      "114/114 [==============================] - 0s 3ms/step\n",
      "89/89 [==============================] - 0s 4ms/step\n",
      "115/115 [==============================] - 0s 4ms/step\n",
      "189/189 [==============================] - 1s 4ms/step\n",
      "137/137 [==============================] - 1s 4ms/step\n",
      "319/319 [==============================] - 1s 4ms/step\n",
      "257/257 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "365/365 [==============================] - 1s 4ms/step\n",
      "306/306 [==============================] - 1s 4ms/step\n",
      "292/292 [==============================] - 1s 4ms/step\n",
      "311/311 [==============================] - 1s 4ms/step\n",
      "192/192 [==============================] - 1s 3ms/step\n",
      "292/292 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "104/104 [==============================] - 0s 3ms/step\n",
      "335/335 [==============================] - 1s 4ms/step\n",
      "236/236 [==============================] - 1s 4ms/step\n",
      "388/388 [==============================] - 1s 4ms/step\n",
      "296/296 [==============================] - 1s 4ms/step\n",
      "308/308 [==============================] - 1s 4ms/step\n",
      "275/275 [==============================] - 1s 4ms/step\n",
      "353/353 [==============================] - 1s 4ms/step\n",
      "302/302 [==============================] - 1s 4ms/step\n",
      "217/217 [==============================] - 1s 4ms/step\n",
      "401/401 [==============================] - 2s 4ms/step\n",
      "170/170 [==============================] - 1s 4ms/step\n",
      "206/206 [==============================] - 1s 4ms/step\n",
      "442/442 [==============================] - 2s 4ms/step\n",
      "191/191 [==============================] - 1s 3ms/step\n",
      "184/184 [==============================] - 1s 4ms/step\n",
      "151/151 [==============================] - 1s 4ms/step\n",
      "219/219 [==============================] - 1s 4ms/step\n",
      "294/294 [==============================] - 1s 4ms/step\n",
      "258/258 [==============================] - 1s 4ms/step\n",
      "326/326 [==============================] - 1s 4ms/step\n",
      "284/284 [==============================] - 1s 4ms/step\n",
      "214/214 [==============================] - 1s 4ms/step\n",
      "145/145 [==============================] - 1s 4ms/step\n",
      "317/317 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "248/248 [==============================] - 1s 4ms/step\n",
      "279/279 [==============================] - 1s 4ms/step\n",
      "155/155 [==============================] - 1s 3ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "260/260 [==============================] - 1s 4ms/step\n",
      "157/157 [==============================] - 1s 4ms/step\n",
      "330/330 [==============================] - 1s 4ms/step\n",
      "287/287 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "213/213 [==============================] - 1s 4ms/step\n",
      "89/89 [==============================] - 0s 3ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "212/212 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "281/281 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "168/168 [==============================] - 1s 4ms/step\n",
      "327/327 [==============================] - 1s 4ms/step\n",
      "320/320 [==============================] - 1s 4ms/step\n",
      "292/292 [==============================] - 1s 4ms/step\n",
      "216/216 [==============================] - 1s 4ms/step\n",
      "117/117 [==============================] - 0s 3ms/step\n",
      "322/322 [==============================] - 1s 4ms/step\n",
      "247/247 [==============================] - 1s 4ms/step\n",
      "288/288 [==============================] - 1s 4ms/step\n",
      "328/328 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "326/326 [==============================] - 1s 4ms/step\n",
      "186/186 [==============================] - 1s 4ms/step\n",
      "342/342 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "281/281 [==============================] - 1s 4ms/step\n",
      "256/256 [==============================] - 1s 4ms/step\n",
      "240/240 [==============================] - 1s 4ms/step\n",
      "186/186 [==============================] - 1s 4ms/step\n",
      "183/183 [==============================] - 1s 4ms/step\n",
      "191/191 [==============================] - 1s 4ms/step\n",
      "156/156 [==============================] - 1s 4ms/step\n",
      "240/240 [==============================] - 1s 4ms/step\n",
      "240/240 [==============================] - 1s 4ms/step\n",
      "91/91 [==============================] - 0s 4ms/step\n",
      "202/202 [==============================] - 1s 4ms/step\n",
      "104/104 [==============================] - 0s 4ms/step\n",
      "305/305 [==============================] - 1s 4ms/step\n",
      "258/258 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "310/310 [==============================] - 1s 4ms/step\n",
      "171/171 [==============================] - 1s 4ms/step\n",
      "187/187 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "310/310 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "356/356 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "275/275 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "312/312 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "262/262 [==============================] - 1s 4ms/step\n",
      "309/309 [==============================] - 1s 4ms/step\n",
      "159/159 [==============================] - 1s 4ms/step\n",
      "206/206 [==============================] - 1s 4ms/step\n",
      "269/269 [==============================] - 1s 4ms/step\n",
      "260/260 [==============================] - 1s 4ms/step\n",
      "240/240 [==============================] - 1s 4ms/step\n",
      "240/240 [==============================] - 1s 4ms/step\n",
      "295/295 [==============================] - 1s 4ms/step\n",
      "338/338 [==============================] - 1s 4ms/step\n",
      "84/84 [==============================] - 0s 3ms/step\n",
      "173/173 [==============================] - 1s 4ms/step\n",
      "117/117 [==============================] - 0s 4ms/step\n",
      "197/197 [==============================] - 1s 4ms/step\n",
      "176/176 [==============================] - 1s 4ms/step\n",
      "104/104 [==============================] - 0s 3ms/step\n",
      "71/71 [==============================] - 0s 3ms/step\n",
      "194/194 [==============================] - 1s 4ms/step\n",
      "126/126 [==============================] - 0s 3ms/step\n",
      "170/170 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "358/358 [==============================] - 1s 4ms/step\n",
      "251/251 [==============================] - 1s 4ms/step\n",
      "312/312 [==============================] - 1s 4ms/step\n",
      "328/328 [==============================] - 1s 4ms/step\n",
      "327/327 [==============================] - 1s 4ms/step\n",
      "280/280 [==============================] - 1s 4ms/step\n",
      "225/225 [==============================] - 1s 4ms/step\n",
      "205/205 [==============================] - 1s 4ms/step\n",
      "304/304 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "307/307 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "200/200 [==============================] - 1s 4ms/step\n",
      "196/196 [==============================] - 1s 4ms/step\n",
      "308/308 [==============================] - 1s 4ms/step\n",
      "312/312 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "338/338 [==============================] - 1s 4ms/step\n",
      "276/276 [==============================] - 1s 3ms/step\n",
      "292/292 [==============================] - 1s 4ms/step\n",
      "267/267 [==============================] - 1s 4ms/step\n",
      "84/84 [==============================] - 0s 3ms/step\n",
      "257/257 [==============================] - 1s 3ms/step\n",
      "240/240 [==============================] - 1s 4ms/step\n",
      "240/240 [==============================] - 1s 4ms/step\n",
      "240/240 [==============================] - 1s 4ms/step\n",
      "240/240 [==============================] - 1s 4ms/step\n",
      "144/144 [==============================] - 1s 4ms/step\n",
      "193/193 [==============================] - 1s 4ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213/213 [==============================] - 1s 4ms/step\n",
      "231/231 [==============================] - 1s 4ms/step\n",
      "178/178 [==============================] - 1s 4ms/step\n",
      "166/166 [==============================] - 1s 3ms/step\n",
      "322/322 [==============================] - 1s 4ms/step\n",
      "163/163 [==============================] - 1s 4ms/step\n",
      "166/166 [==============================] - 1s 4ms/step\n",
      "206/206 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "299/299 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "192/192 [==============================] - 1s 4ms/step\n",
      "228/228 [==============================] - 1s 4ms/step\n",
      "265/265 [==============================] - 1s 4ms/step\n",
      "252/252 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 3ms/step\n",
      "313/313 [==============================] - 1s 3ms/step\n",
      "311/311 [==============================] - 1s 4ms/step\n",
      "200/200 [==============================] - 1s 3ms/step\n",
      "81/81 [==============================] - 0s 3ms/step\n",
      "315/315 [==============================] - 1s 3ms/step\n",
      "316/316 [==============================] - 1s 3ms/step\n",
      "248/248 [==============================] - 1s 3ms/step\n",
      "314/314 [==============================] - 1s 3ms/step\n",
      "194/194 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "230/230 [==============================] - 1s 3ms/step\n",
      "141/141 [==============================] - 0s 3ms/step\n",
      "376/376 [==============================] - 1s 3ms/step\n",
      "293/293 [==============================] - 1s 4ms/step\n",
      "289/289 [==============================] - 1s 4ms/step\n",
      "262/262 [==============================] - 1s 4ms/step\n",
      "261/261 [==============================] - 1s 3ms/step\n",
      "272/272 [==============================] - 1s 4ms/step\n",
      "243/243 [==============================] - 1s 3ms/step\n",
      "200/200 [==============================] - 1s 4ms/step\n",
      "257/257 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "214/214 [==============================] - 1s 3ms/step\n",
      "293/293 [==============================] - 1s 3ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "324/324 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "201/201 [==============================] - 1s 4ms/step\n",
      "341/341 [==============================] - 1s 4ms/step\n",
      "232/232 [==============================] - 1s 4ms/step\n",
      "249/249 [==============================] - 1s 4ms/step\n",
      "240/240 [==============================] - 1s 4ms/step\n",
      "145/145 [==============================] - 1s 3ms/step\n",
      "97/97 [==============================] - 0s 3ms/step\n",
      "171/171 [==============================] - 1s 4ms/step\n",
      "238/238 [==============================] - 1s 4ms/step\n",
      "228/228 [==============================] - 1s 4ms/step\n",
      "230/230 [==============================] - 1s 4ms/step\n",
      "157/157 [==============================] - 1s 4ms/step\n",
      "235/235 [==============================] - 1s 4ms/step\n",
      "235/235 [==============================] - 1s 4ms/step\n",
      "234/234 [==============================] - 1s 4ms/step\n",
      "234/234 [==============================] - 1s 4ms/step\n",
      "234/234 [==============================] - 1s 4ms/step\n",
      "234/234 [==============================] - 1s 4ms/step\n",
      "234/234 [==============================] - 1s 4ms/step\n",
      "235/235 [==============================] - 1s 3ms/step\n",
      "225/225 [==============================] - 1s 4ms/step\n",
      "234/234 [==============================] - 1s 4ms/step\n",
      "234/234 [==============================] - 1s 4ms/step\n",
      "234/234 [==============================] - 1s 4ms/step\n",
      "198/198 [==============================] - 1s 4ms/step\n",
      "234/234 [==============================] - 1s 4ms/step\n",
      "234/234 [==============================] - 1s 4ms/step\n",
      "234/234 [==============================] - 1s 4ms/step\n",
      "199/199 [==============================] - 1s 3ms/step\n",
      "234/234 [==============================] - 1s 4ms/step\n",
      "274/274 [==============================] - 1s 4ms/step\n",
      "151/151 [==============================] - 1s 4ms/step\n",
      "233/233 [==============================] - 1s 4ms/step\n",
      "259/259 [==============================] - 1s 4ms/step\n",
      "166/166 [==============================] - 1s 4ms/step\n",
      "169/169 [==============================] - 1s 4ms/step\n",
      "200/200 [==============================] - 1s 4ms/step\n",
      "185/185 [==============================] - 1s 4ms/step\n",
      "254/254 [==============================] - 1s 4ms/step\n",
      "201/201 [==============================] - 1s 4ms/step\n",
      "221/221 [==============================] - 1s 4ms/step\n",
      "161/161 [==============================] - 1s 4ms/step\n",
      "240/240 [==============================] - 1s 4ms/step\n",
      "227/227 [==============================] - 1s 4ms/step\n",
      "312/312 [==============================] - 1s 4ms/step\n",
      "164/164 [==============================] - 1s 4ms/step\n",
      "307/307 [==============================] - 1s 4ms/step\n",
      "134/134 [==============================] - 0s 3ms/step\n",
      "233/233 [==============================] - 1s 3ms/step\n",
      "349/349 [==============================] - 1s 4ms/step\n",
      "229/229 [==============================] - 1s 4ms/step\n",
      "253/253 [==============================] - 1s 4ms/step\n",
      "277/277 [==============================] - 1s 4ms/step\n",
      "240/240 [==============================] - 1s 4ms/step\n",
      "246/246 [==============================] - 1s 4ms/step\n",
      "204/204 [==============================] - 1s 4ms/step\n",
      "159/159 [==============================] - 1s 4ms/step\n",
      "240/240 [==============================] - 1s 4ms/step\n",
      "258/258 [==============================] - 1s 4ms/step\n",
      "216/216 [==============================] - 1s 4ms/step\n",
      "247/247 [==============================] - 1s 4ms/step\n",
      "230/230 [==============================] - 1s 4ms/step\n",
      "283/283 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "288/288 [==============================] - 1s 4ms/step\n",
      "292/292 [==============================] - 1s 4ms/step\n",
      "315/315 [==============================] - 1s 4ms/step\n",
      "190/190 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "179/179 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "309/309 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "252/252 [==============================] - 1s 4ms/step\n",
      "260/260 [==============================] - 1s 4ms/step\n",
      "231/231 [==============================] - 1s 4ms/step\n",
      "273/273 [==============================] - 1s 4ms/step\n",
      "255/255 [==============================] - 1s 4ms/step\n",
      "294/294 [==============================] - 1s 4ms/step\n",
      "200/200 [==============================] - 1s 4ms/step\n",
      "230/230 [==============================] - 1s 4ms/step\n",
      "295/295 [==============================] - 1s 4ms/step\n",
      "350/350 [==============================] - 1s 4ms/step\n",
      "264/264 [==============================] - 1s 4ms/step\n",
      "160/160 [==============================] - 1s 4ms/step\n",
      "240/240 [==============================] - 1s 4ms/step\n",
      "288/288 [==============================] - 1s 4ms/step\n",
      "209/209 [==============================] - 1s 4ms/step\n",
      "282/282 [==============================] - 1s 4ms/step\n",
      "240/240 [==============================] - 1s 4ms/step\n",
      "240/240 [==============================] - 1s 4ms/step\n",
      "247/247 [==============================] - 1s 4ms/step\n",
      "240/240 [==============================] - 1s 4ms/step\n",
      "240/240 [==============================] - 1s 4ms/step\n",
      "240/240 [==============================] - 1s 4ms/step\n",
      "240/240 [==============================] - 1s 4ms/step\n",
      "240/240 [==============================] - 1s 4ms/step\n",
      "240/240 [==============================] - 1s 4ms/step\n",
      "240/240 [==============================] - 1s 4ms/step\n",
      "254/254 [==============================] - 1s 4ms/step\n",
      "240/240 [==============================] - 1s 4ms/step\n",
      "240/240 [==============================] - 1s 4ms/step\n",
      "164/164 [==============================] - 1s 4ms/step\n",
      "240/240 [==============================] - 1s 4ms/step\n",
      "240/240 [==============================] - 1s 4ms/step\n",
      "118/118 [==============================] - 0s 3ms/step\n",
      "183/183 [==============================] - 1s 4ms/step\n",
      "320/320 [==============================] - 1s 4ms/step\n",
      "185/185 [==============================] - 1s 4ms/step\n",
      "207/207 [==============================] - 1s 4ms/step\n",
      "286/286 [==============================] - 1s 4ms/step\n",
      "299/299 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "175/175 [==============================] - 1s 3ms/step\n",
      "217/217 [==============================] - 1s 4ms/step\n",
      "263/263 [==============================] - 1s 4ms/step\n",
      "235/235 [==============================] - 1s 4ms/step\n",
      "232/232 [==============================] - 1s 4ms/step\n",
      "240/240 [==============================] - 1s 4ms/step\n",
      "231/231 [==============================] - 1s 4ms/step\n",
      "240/240 [==============================] - 1s 4ms/step\n",
      "199/199 [==============================] - 1s 4ms/step\n",
      "184/184 [==============================] - 1s 4ms/step\n",
      "240/240 [==============================] - 1s 4ms/step\n",
      "326/326 [==============================] - 1s 4ms/step\n",
      "240/240 [==============================] - 1s 4ms/step\n",
      "153/153 [==============================] - 1s 4ms/step\n",
      "151/151 [==============================] - 1s 4ms/step\n",
      "320/320 [==============================] - 1s 4ms/step\n",
      "338/338 [==============================] - 1s 4ms/step\n",
      "359/359 [==============================] - 1s 4ms/step\n",
      "140/140 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "300/300 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "177/177 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "263/263 [==============================] - 1s 4ms/step\n",
      "307/307 [==============================] - 1s 4ms/step\n",
      "301/301 [==============================] - 1s 4ms/step\n",
      "175/175 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "324/324 [==============================] - 1s 4ms/step\n",
      "362/362 [==============================] - 1s 4ms/step\n",
      "247/247 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "202/202 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "267/267 [==============================] - 1s 4ms/step\n",
      "206/206 [==============================] - 1s 3ms/step\n",
      "281/281 [==============================] - 1s 4ms/step\n",
      "312/312 [==============================] - 1s 4ms/step\n",
      "284/284 [==============================] - 1s 3ms/step\n",
      "338/338 [==============================] - 1s 4ms/step\n",
      "345/345 [==============================] - 1s 4ms/step\n",
      "303/303 [==============================] - 1s 4ms/step\n",
      "195/195 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "255/255 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "335/335 [==============================] - 1s 4ms/step\n",
      "174/174 [==============================] - 1s 4ms/step\n",
      "175/175 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "349/349 [==============================] - 1s 4ms/step\n",
      "363/363 [==============================] - 1s 4ms/step\n",
      "320/320 [==============================] - 1s 4ms/step\n",
      "200/200 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "313/313 [==============================] - 1s 4ms/step\n",
      "379/379 [==============================] - 1s 4ms/step\n",
      "330/330 [==============================] - 1s 4ms/step\n",
      "203/203 [==============================] - 1s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "X_TEST = load_data_testing(test_data_dir)\n",
    "X_TEST = np.array(X_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e02c05c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame(data=X_TEST, columns=[\"P\", \"K\", \"Mg\", \"pH\"])\n",
    "data = submission_df\n",
    "# Create new columns for labels and values\n",
    "labels = []\n",
    "values = []\n",
    "\n",
    "# Iterate through each row of the original data and expand it\n",
    "for i, row in data.iterrows():\n",
    "    for j, col in row.iteritems():\n",
    "        labels.append(f\"{i}_{j}\")\n",
    "        values.append(col)\n",
    "\n",
    "# Create a new DataFrame with the expanded data\n",
    "new_data = pd.DataFrame({\"Label\": labels, \"Value\": values})\n",
    "\n",
    "new_data.to_csv(\"SampleSubmission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
